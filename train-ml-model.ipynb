{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Imports\n",
    "import csv\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "# Movenet Thunder and Python Pose Estimation\n",
    "sys.path.append('./TF-Movenet/examples/lite/examples/pose_estimation/raspberry_pi')\n",
    "\n",
    "import utils\n",
    "from data import BodyPart\n",
    "from ml import Movenet\n",
    "\n",
    "movenet = Movenet('./TF-Movenet/movenet_thunder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Person Within Input Image\n",
    "def detect(input_tensor, inference_count=3):\n",
    "    image_height, image_width, channel = input_tensor.shape\n",
    "\n",
    "    movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
    "\n",
    "    for _ in range(inference_count - 1):\n",
    "        person = movenet.detect(input_tensor.numpy(), reset_crop_region=False)\n",
    "\n",
    "    return person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Pose Estimation on Image\n",
    "def draw_prediction_on_image(image, person, crop_region=None, close_figure=True, keep_input_size=False):\n",
    "    image_np = utils.visualize(image, [person])\n",
    "\n",
    "    height, width, channel = image.shape\n",
    "    aspect_ratio = float(width) / height\n",
    "    fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "    im = ax.imshow(image_np)\n",
    "\n",
    "    if close_figure:\n",
    "        plt.close(fig)\n",
    "\n",
    "    if not keep_input_size:\n",
    "        image_np = utils.keep_aspect_ratio_resizer(image_np, (512, 512))\n",
    "\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveNetPreprocessor(object):\n",
    "\n",
    "    def __init__(self, images_in_folder, images_out_folder, csvs_out_path):\n",
    "        self._images_in_folder = images_in_folder\n",
    "        self._images_out_folder = images_out_folder\n",
    "        self._csvs_out_path = csvs_out_path\n",
    "        self._messages = []\n",
    "\n",
    "        # Create a temp dir to store the pose CSVs per class\n",
    "        self._csvs_out_folder_per_class = tempfile.mkdtemp()\n",
    "\n",
    "        # Get list of pose classes and print image statistics\n",
    "        self._pose_class_names = sorted([n for n in os.listdir(self._images_in_folder) if not n.startswith('.')])\n",
    "\n",
    "    # Preprocess Images in Given Folder\n",
    "    def process(self, per_pose_class_limit=None, detection_threshold=0.1):\n",
    "\n",
    "        # Loop through the classes and preprocess its images\n",
    "        for pose_class_name in self._pose_class_names:\n",
    "            print('Preprocessing', pose_class_name, file=sys.stderr)\n",
    "\n",
    "            # Paths for the pose class.\n",
    "            images_in_folder = os.path.join(self._images_in_folder, pose_class_name)\n",
    "            images_out_folder = os.path.join(self._images_out_folder, pose_class_name)\n",
    "            csv_out_path = os.path.join(self._csvs_out_folder_per_class, pose_class_name + '.csv')\n",
    "            if not os.path.exists(images_out_folder):\n",
    "                os.makedirs(images_out_folder)\n",
    "\n",
    "            # Detect landmarks in each image and write it to a CSV file\n",
    "            with open(csv_out_path, 'w') as csv_out_file:\n",
    "                csv_out_writer = csv.writer(csv_out_file, \n",
    "                                            delimiter=',', \n",
    "                                            quoting=csv.QUOTE_MINIMAL)\n",
    "                # Get list of images\n",
    "                image_names = sorted([n for n in os.listdir(images_in_folder) if not n.startswith('.')])\n",
    "                if per_pose_class_limit is not None:\n",
    "                    image_names = image_names[:per_pose_class_limit]\n",
    "\n",
    "                valid_image_count = 0\n",
    "\n",
    "                # Detect pose landmarks from each image\n",
    "                for image_name in tqdm.tqdm(image_names):\n",
    "                    image_path = os.path.join(images_in_folder, image_name)\n",
    "\n",
    "                    try:\n",
    "                        image = tf.io.read_file(image_path)\n",
    "                        image = tf.io.decode_jpeg(image)\n",
    "                    except:\n",
    "                        self._messages.append('Skipped ' + image_path + '. Invalid image.')\n",
    "                        continue\n",
    "                    else:\n",
    "                        image = tf.io.read_file(image_path)\n",
    "                        image = tf.io.decode_jpeg(image)\n",
    "                        image_height, image_width, channel = image.shape\n",
    "\n",
    "                    # Skip images that isn't RGB because Movenet requires RGB images\n",
    "                    if channel != 3:\n",
    "                        self._messages.append('Skipped ' + image_path + '. Image isn\\'t in RGB format.')\n",
    "                        continue\n",
    "                    person = detect(image)\n",
    "\n",
    "                    # Save landmarks if all landmarks were detected\n",
    "                    min_landmark_score = min(\n",
    "                        [keypoint.score for keypoint in person.keypoints])\n",
    "                    should_keep_image = min_landmark_score >= detection_threshold\n",
    "                    if not should_keep_image:\n",
    "                        self._messages.append('Skipped ' + image_path + '. No pose was confidently detected.')\n",
    "                        continue\n",
    "                        \n",
    "                    valid_image_count += 1\n",
    "\n",
    "                    # Draw the prediction result on top of the image for debugging later\n",
    "                    output_overlay = draw_prediction_on_image(\n",
    "                        image.numpy().astype(np.uint8), person, \n",
    "                        close_figure=True, keep_input_size=True)\n",
    "\n",
    "                    # Write detection result into an image file\n",
    "                    output_frame = cv2.cvtColor(output_overlay, cv2.COLOR_RGB2BGR)\n",
    "                    cv2.imwrite(os.path.join(images_out_folder, image_name), output_frame)\n",
    "\n",
    "                    # Get landmarks and scale it to the same size as the input image\n",
    "                    pose_landmarks = np.array(\n",
    "                        [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score]\n",
    "                            for keypoint in person.keypoints],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "                    # Write the landmark coordinates to its per-class CSV file\n",
    "                    coordinates = pose_landmarks.flatten().astype(np.str).tolist()\n",
    "                    csv_out_writer.writerow([image_name] + coordinates)\n",
    "\n",
    "                if not valid_image_count:\n",
    "                    raise RuntimeError(\n",
    "                        'No valid images found for the \"{}\" class.'\n",
    "                        .format(pose_class_name))\n",
    "\n",
    "        # Print the error message collected during preprocessing.\n",
    "        print('\\n'.join(self._messages))\n",
    "\n",
    "        # Combine all per-class CSVs into a single output file\n",
    "        all_landmarks_df = self._all_landmarks_as_dataframe()\n",
    "        all_landmarks_df.to_csv(self._csvs_out_path, index=False)\n",
    "\n",
    "    # List Classes in Training Dataset\n",
    "    def class_names(self):\n",
    "        return self._pose_class_names\n",
    "\n",
    "    # Merging All CSVs for Each Class into Single Dataframe\n",
    "    def _all_landmarks_as_dataframe(self):\n",
    "        total_df = None\n",
    "\n",
    "        for class_index, class_name in enumerate(self._pose_class_names):\n",
    "            csv_out_path = os.path.join(self._csvs_out_folder_per_class, class_name + '.csv')\n",
    "            per_class_df = pd.read_csv(csv_out_path, header=None)\n",
    "\n",
    "            # Add the labels\n",
    "            per_class_df['class_no'] = [class_index]*len(per_class_df)\n",
    "            per_class_df['class_name'] = [class_name]*len(per_class_df)\n",
    "\n",
    "            # Append the folder name to the filename column (first column)\n",
    "            per_class_df[per_class_df.columns[0]] = (os.path.join(class_name, '') +\n",
    "                                                    per_class_df[per_class_df.columns[0]].astype(str))\n",
    "\n",
    "            if total_df is None:\n",
    "                # For the first class, assign its data to the total dataframe\n",
    "                total_df = per_class_df\n",
    "            else:\n",
    "                # Concatenate each class's data into the total dataframe\n",
    "                total_df = pd.concat([total_df, per_class_df], axis=0)\n",
    "\n",
    "        list_name = [[bodypart.name + '_x', bodypart.name + '_y', \n",
    "                    bodypart.name + '_score'] for bodypart in BodyPart] \n",
    "\n",
    "        header_name = []\n",
    "        for columns_name in list_name:\n",
    "            header_name += columns_name\n",
    "\n",
    "        header_name = ['file_name'] + header_name\n",
    "        header_map = {total_df.columns[i]: header_name[i] for i in range(len(header_name))}\n",
    "\n",
    "        total_df.rename(header_map, axis=1, inplace=True)\n",
    "\n",
    "        return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Directory into Train and Test Images\n",
    "def split_into_train_test(images_origin, images_dest, test_split):\n",
    "    _, dirs, _ = next(os.walk(images_origin))\n",
    "\n",
    "    TRAIN_DIR = os.path.join(images_dest, 'train')\n",
    "    TEST_DIR = os.path.join(images_dest, 'test')\n",
    "    os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "    os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "    for dir in dirs:\n",
    "        # Get all filenames for this dir, filtered by filetype\n",
    "        filenames = os.listdir(os.path.join(images_origin, dir))\n",
    "        filenames = [os.path.join(images_origin, dir, f) for f in filenames if (\n",
    "            f.endswith('.png') or f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.bmp'))]\n",
    "\n",
    "        # Shuffle the files, deterministically\n",
    "        filenames.sort()\n",
    "        random.seed(42)\n",
    "        random.shuffle(filenames)\n",
    "\n",
    "        # Divide them into train/test dirs\n",
    "        os.makedirs(os.path.join(TEST_DIR, dir), exist_ok=True)\n",
    "        os.makedirs(os.path.join(TRAIN_DIR, dir), exist_ok=True)\n",
    "        test_count = int(len(filenames) * test_split)\n",
    "        for i, file in enumerate(filenames):\n",
    "            if i < test_count:\n",
    "                destination = os.path.join(TEST_DIR, dir, os.path.split(file)[1])\n",
    "            else:\n",
    "                destination = os.path.join(TRAIN_DIR, dir, os.path.split(file)[1])\n",
    "            shutil.copyfile(file, destination)\n",
    "\n",
    "        print(f'Moved {test_count} of {len(filenames)} from class \"{dir}\" into test.')\n",
    "\n",
    "    print(f'Your split dataset is in \"{images_dest}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set/Check Path/File Variables Before Running\n",
    "DATASET_IN = \"./Input/Handstand/\"\n",
    "DATASET_OUT = \"./Input/Split-Handstand/\"\n",
    "OUTPUT_DIR = \"./TF-Models/Handstand/Data/\"\n",
    "CLASSIFIER = \"./TF-Models/Handstand/Handstand-Classifier.tflite\"\n",
    "LABELS = \"./TF-Models/Handstand/Handstand-Labels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 135 of 678 from class \"Bad\" into test.\n",
      "Moved 31 of 156 from class \"Good\" into test.\n",
      "Moved 52 of 264 from class \"Average\" into test.\n",
      "Your split dataset is in \"./Input/Split-Handstand/\"\n"
     ]
    }
   ],
   "source": [
    "# Splitting the Dataset\n",
    "dataset_in = DATASET_IN\n",
    "dataset_out = DATASET_OUT\n",
    "split_into_train_test(dataset_in, dataset_out, test_split=0.2)\n",
    "IMAGES_ROOT = dataset_out\n",
    "\n",
    "output_dir = OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Average\n",
      "  0%|          | 0/212 [00:00<?, ?it/s]/Users/rahul/Documents/Calisthenics-App/venv/lib/python3.7/site-packages/ipykernel_launcher.py:88: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "100%|██████████| 212/212 [02:05<00:00,  1.69it/s]\n",
      "Preprocessing Bad\n",
      "100%|██████████| 578/578 [03:37<00:00,  2.66it/s]\n",
      "Preprocessing Good\n",
      "100%|██████████| 126/126 [05:16<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped ./Input/Split-Handstand/train/Average/156.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Average/248.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Average/32.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Average/61.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/187.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/188.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/19.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/21.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/216.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/22.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/220.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/23.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/234.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/235.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/27.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/276.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/278.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/28.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/29.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/304.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/325.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/429.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/431.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/442.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/46.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/47.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/48.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/49.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/51.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/52.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/53.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/535.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/543.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/547.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/583.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/627.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/628.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/629.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/636.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Bad/639.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Good/108.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Good/109.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Good/112.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Good/145.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Good/146.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/train/Good/69.jpeg. No pose was confidently detected.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Train Folder\n",
    "images_in_train_folder = os.path.join(IMAGES_ROOT, 'train')\n",
    "images_out_train_folder = output_dir + '/images_out_train'\n",
    "csvs_out_train_path = output_dir + '/train_data.csv'\n",
    "\n",
    "preprocessor = MoveNetPreprocessor(\n",
    "    images_in_folder=images_in_train_folder,\n",
    "    images_out_folder=images_out_train_folder,\n",
    "    csvs_out_path=csvs_out_train_path,\n",
    ")\n",
    "preprocessor.process(per_pose_class_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Average\n",
      "  4%|▍         | 2/52 [00:03<01:27,  1.74s/it]/Users/rahul/Documents/Calisthenics-App/venv/lib/python3.7/site-packages/ipykernel_launcher.py:88: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "100%|██████████| 52/52 [00:38<00:00,  1.35it/s]\n",
      "Preprocessing Bad\n",
      "100%|██████████| 174/174 [02:21<00:00,  1.23it/s]\n",
      "Preprocessing Good\n",
      "100%|██████████| 32/32 [00:45<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped ./Input/Split-Handstand/test/Average/100.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Average/101.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Average/157.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Average/60.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Average/62.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Average/99.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Bad/19.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Bad/217.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Bad/220.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Bad/27.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Bad/51.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Bad/52.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Bad/54.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Bad/625.jpeg. No pose was confidently detected.\n",
      "Skipped ./Input/Split-Handstand/test/Good/70.jpeg. No pose was confidently detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Test Folder\n",
    "images_in_test_folder = os.path.join(IMAGES_ROOT, 'test')\n",
    "images_out_test_folder = output_dir + '/images_out_test'\n",
    "csvs_out_test_path = output_dir + '/test_data.csv'\n",
    "\n",
    "preprocessor = MoveNetPreprocessor(\n",
    "    images_in_folder=images_in_test_folder,\n",
    "    images_out_folder=images_out_test_folder,\n",
    "    csvs_out_path=csvs_out_test_path,\n",
    ")\n",
    "preprocessor.process(per_pose_class_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a CSV Created by MoveNetPreprocessor\n",
    "def load_pose_landmarks(csv_path):\n",
    "    # Load the CSV file\n",
    "    dataframe = pd.read_csv(csv_path)\n",
    "    df_to_process = dataframe.copy()\n",
    "\n",
    "    # Drop the file_name columns as you don't need it during training.\n",
    "    df_to_process.drop(columns=['file_name'], inplace=True)\n",
    "\n",
    "    # Extract the list of class names\n",
    "    classes = df_to_process.pop('class_name').unique()\n",
    "\n",
    "    # Extract the labels\n",
    "    y = df_to_process.pop('class_no')\n",
    "\n",
    "    # Convert the input features and labels into the correct format for training.\n",
    "    X = df_to_process.astype('float64')\n",
    "    y = keras.utils.to_categorical(y)\n",
    "\n",
    "    return X, y, classes, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train data\n",
    "X, y, class_names, _ = load_pose_landmarks(csvs_out_train_path)\n",
    "\n",
    "# Split training data (X, y) into (X_train, y_train) and (X_val, y_val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "# Load the test data\n",
    "X_test, y_test, _, df_test = load_pose_landmarks(csvs_out_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Center Point of Two Given Landmarks\n",
    "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
    "  left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
    "  right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
    "  center = left * 0.5 + right * 0.5\n",
    "  return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Pose Size \n",
    "# Maximum of two values \n",
    "# - Torso size multiplied by `torso_size_multiplier` \n",
    "# - Maximum distance from pose center to any pose landmark\n",
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "  \n",
    "    # Hips center\n",
    "    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, BodyPart.RIGHT_HIP)\n",
    "\n",
    "    # Shoulders center\n",
    "    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER, BodyPart.RIGHT_SHOULDER)\n",
    "\n",
    "    # Torso size as the minimum body size\n",
    "    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "\n",
    "    # Pose center\n",
    "    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, BodyPart.RIGHT_HIP)\n",
    "    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "\n",
    "    # Broadcast the pose center to the same size as the landmark vector to\n",
    "    # perform substraction\n",
    "    pose_center_new = tf.broadcast_to(pose_center_new, [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "    # Dist to pose center\n",
    "    d = tf.gather(landmarks - pose_center_new, 0, axis=0, name=\"dist_to_pose_center\")\n",
    "\n",
    "    # Max dist to pose center\n",
    "    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "    # Normalize scale\n",
    "    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "\n",
    "    return pose_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Landmarks by Moving Pose Center to (0,0) and Scaling to Constant Size\n",
    "def normalize_pose_landmarks(landmarks):\n",
    "    # Move landmarks so that the pose center becomes (0,0)\n",
    "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, BodyPart.RIGHT_HIP)\n",
    "\n",
    "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to perform\n",
    "    # substraction\n",
    "    pose_center = tf.broadcast_to(pose_center, [tf.size(landmarks) // (17*2), 17, 2])\n",
    "    landmarks = landmarks - pose_center\n",
    "\n",
    "    # Scale the landmarks to a constant pose size\n",
    "    pose_size = get_pose_size(landmarks)\n",
    "    landmarks /= pose_size\n",
    "\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts Input Landmarks into Pose Embedding\n",
    "def landmarks_to_embedding(landmarks_and_scores):\n",
    "\n",
    "  # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "  reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "  # Normalize landmarks 2D\n",
    "  landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n",
    "\n",
    "  # Flatten the normalized landmark coordinates into a vector\n",
    "  embedding = keras.layers.Flatten()(landmarks)\n",
    "\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 51)]         0           []                               \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 17, 3)        0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 17, 2)       0           ['reshape_1[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_9 (TFOpLam  (None, 2)           0           ['tf.__operators__.getitem_1[0][0\n",
      " bda)                                                            ]']                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_10 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_1[0][0\n",
      " mbda)                                                           ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_9[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_10 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_10[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 2)           0           ['tf.math.multiply_9[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_10[0][0]']    \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_2 (TFOpLambd  ()                  0           ['tf.__operators__.getitem_1[0][0\n",
      " a)                                                              ]']                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLambda)  (None, 1, 2)         0           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_2 (TFOp  ()                  0           ['tf.compat.v1.size_2[0][0]']    \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.broadcast_to_2 (TFOpLambda)  (None, 17, 2)       0           ['tf.expand_dims_2[0][0]',       \n",
      "                                                                  'tf.compat.v1.floor_div_2[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.subtract_3 (TFOpLambda  (None, 17, 2)       0           ['tf.__operators__.getitem_1[0][0\n",
      " )                                                               ]',                              \n",
      "                                                                  'tf.broadcast_to_2[0][0]']      \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_15 (TFOpLa  (None, 2)           0           ['tf.math.subtract_3[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_16 (TFOpLa  (None, 2)           0           ['tf.math.subtract_3[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_15 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_15[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_16 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_16[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 2)           0           ['tf.math.multiply_15[0][0]',    \n",
      " mbda)                                                            'tf.math.multiply_16[0][0]']    \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_3 (TFOpLambd  ()                  0           ['tf.math.subtract_3[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_13 (TFOpLa  (None, 2)           0           ['tf.math.subtract_3[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_14 (TFOpLa  (None, 2)           0           ['tf.math.subtract_3[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_11 (TFOpLa  (None, 2)           0           ['tf.math.subtract_3[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_12 (TFOpLa  (None, 2)           0           ['tf.math.subtract_3[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.expand_dims_3 (TFOpLambda)  (None, 1, 2)         0           ['tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_3 (TFOp  ()                  0           ['tf.compat.v1.size_3[0][0]']    \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.multiply_13 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_13[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_14 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_14[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_11 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_11[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_12 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_12[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.broadcast_to_3 (TFOpLambda)  (None, 17, 2)       0           ['tf.expand_dims_3[0][0]',       \n",
      "                                                                  'tf.compat.v1.floor_div_3[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 2)           0           ['tf.math.multiply_13[0][0]',    \n",
      " mbda)                                                            'tf.math.multiply_14[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 2)           0           ['tf.math.multiply_11[0][0]',    \n",
      " mbda)                                                            'tf.math.multiply_12[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.subtract_5 (TFOpLambda  (None, 17, 2)       0           ['tf.math.subtract_3[0][0]',     \n",
      " )                                                                'tf.broadcast_to_3[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.subtract_4 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_6[0][0]', \n",
      " )                                                                'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_17 (TFOpLa  (17, 2)             0           ['tf.math.subtract_5[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_2 (TFOpLambd  ()                  0           ['tf.math.subtract_4[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_3 (TFOpLambd  (2,)                0           ['tf.compat.v1.gather_17[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_17 (TFOpLambd  ()                  0           ['tf.compat.v1.norm_2[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_1 (TFOpLamb  ()                  0           ['tf.compat.v1.norm_3[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.maximum_1 (TFOpLambda)  ()                  0           ['tf.math.multiply_17[0][0]',    \n",
      "                                                                  'tf.math.reduce_max_1[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.truediv_1 (TFOpLambda)  (None, 17, 2)       0           ['tf.math.subtract_3[0][0]',     \n",
      "                                                                  'tf.math.maximum_1[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 34)           0           ['tf.math.truediv_1[0][0]']      \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          4480        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3)            195         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,931\n",
      "Trainable params: 12,931\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "inputs = tf.keras.Input(shape=(51))\n",
    "embedding = landmarks_to_embedding(inputs)\n",
    "\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "35/47 [=====================>........] - ETA: 0s - loss: 0.9991 - accuracy: 0.7304\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68702, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 2s 16ms/step - loss: 0.9570 - accuracy: 0.7361 - val_loss: 0.7982 - val_accuracy: 0.6870\n",
      "Epoch 2/200\n",
      "35/47 [=====================>........] - ETA: 0s - loss: 0.6345 - accuracy: 0.7679\n",
      "Epoch 2: val_accuracy improved from 0.68702 to 0.70229, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6101 - accuracy: 0.7659 - val_loss: 0.5401 - val_accuracy: 0.7023\n",
      "Epoch 3/200\n",
      "35/47 [=====================>........] - ETA: 0s - loss: 0.4814 - accuracy: 0.7964\n",
      "Epoch 3: val_accuracy improved from 0.70229 to 0.80916, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4624 - accuracy: 0.7997 - val_loss: 0.4159 - val_accuracy: 0.8092\n",
      "Epoch 4/200\n",
      "38/47 [=======================>......] - ETA: 0s - loss: 0.3782 - accuracy: 0.8322\n",
      "Epoch 4: val_accuracy improved from 0.80916 to 0.87023, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3695 - accuracy: 0.8376 - val_loss: 0.3330 - val_accuracy: 0.8702\n",
      "Epoch 5/200\n",
      "34/47 [====================>.........] - ETA: 0s - loss: 0.3221 - accuracy: 0.8768\n",
      "Epoch 5: val_accuracy improved from 0.87023 to 0.93130, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8890 - val_loss: 0.2835 - val_accuracy: 0.9313\n",
      "Epoch 6/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.2875 - accuracy: 0.8902\n",
      "Epoch 6: val_accuracy improved from 0.93130 to 0.93893, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2778 - accuracy: 0.9012 - val_loss: 0.2601 - val_accuracy: 0.9389\n",
      "Epoch 7/200\n",
      "34/47 [====================>.........] - ETA: 0s - loss: 0.2541 - accuracy: 0.9246\n",
      "Epoch 7: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2608 - accuracy: 0.9147 - val_loss: 0.2236 - val_accuracy: 0.9313\n",
      "Epoch 8/200\n",
      "34/47 [====================>.........] - ETA: 0s - loss: 0.2211 - accuracy: 0.9265\n",
      "Epoch 8: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2341 - accuracy: 0.9093 - val_loss: 0.2157 - val_accuracy: 0.9389\n",
      "Epoch 9/200\n",
      "38/47 [=======================>......] - ETA: 0s - loss: 0.1951 - accuracy: 0.9474\n",
      "Epoch 9: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2098 - accuracy: 0.9378 - val_loss: 0.1986 - val_accuracy: 0.9389\n",
      "Epoch 10/200\n",
      "30/47 [==================>...........] - ETA: 0s - loss: 0.2114 - accuracy: 0.9250\n",
      "Epoch 10: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9283 - val_loss: 0.1924 - val_accuracy: 0.9313\n",
      "Epoch 11/200\n",
      "38/47 [=======================>......] - ETA: 0s - loss: 0.1854 - accuracy: 0.9424\n",
      "Epoch 11: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1997 - accuracy: 0.9391 - val_loss: 0.1769 - val_accuracy: 0.9389\n",
      "Epoch 12/200\n",
      "33/47 [====================>.........] - ETA: 0s - loss: 0.1750 - accuracy: 0.9432\n",
      "Epoch 12: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1922 - accuracy: 0.9323 - val_loss: 0.1868 - val_accuracy: 0.9313\n",
      "Epoch 13/200\n",
      "35/47 [=====================>........] - ETA: 0s - loss: 0.1449 - accuracy: 0.9589\n",
      "Epoch 13: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1554 - accuracy: 0.9526 - val_loss: 0.1797 - val_accuracy: 0.9389\n",
      "Epoch 14/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.1929 - accuracy: 0.9240\n",
      "Epoch 14: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1910 - accuracy: 0.9296 - val_loss: 0.1721 - val_accuracy: 0.9389\n",
      "Epoch 15/200\n",
      "34/47 [====================>.........] - ETA: 0s - loss: 0.1493 - accuracy: 0.9504\n",
      "Epoch 15: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1572 - accuracy: 0.9499 - val_loss: 0.1619 - val_accuracy: 0.9389\n",
      "Epoch 16/200\n",
      "36/47 [=====================>........] - ETA: 0s - loss: 0.1548 - accuracy: 0.9566\n",
      "Epoch 16: val_accuracy did not improve from 0.93893\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1542 - accuracy: 0.9553 - val_loss: 0.1709 - val_accuracy: 0.9389\n",
      "Epoch 17/200\n",
      "36/47 [=====================>........] - ETA: 0s - loss: 0.1274 - accuracy: 0.9618\n",
      "Epoch 17: val_accuracy improved from 0.93893 to 0.94656, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1318 - accuracy: 0.9581 - val_loss: 0.1675 - val_accuracy: 0.9466\n",
      "Epoch 18/200\n",
      "38/47 [=======================>......] - ETA: 0s - loss: 0.1381 - accuracy: 0.9523\n",
      "Epoch 18: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9526 - val_loss: 0.1638 - val_accuracy: 0.9389\n",
      "Epoch 19/200\n",
      "34/47 [====================>.........] - ETA: 0s - loss: 0.1477 - accuracy: 0.9540\n",
      "Epoch 19: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1684 - accuracy: 0.9418 - val_loss: 0.1462 - val_accuracy: 0.9466\n",
      "Epoch 20/200\n",
      "44/47 [===========================>..] - ETA: 0s - loss: 0.1440 - accuracy: 0.9517\n",
      "Epoch 20: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1420 - accuracy: 0.9526 - val_loss: 0.1476 - val_accuracy: 0.9466\n",
      "Epoch 21/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.1223 - accuracy: 0.9679\n",
      "Epoch 21: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9662 - val_loss: 0.1552 - val_accuracy: 0.9466\n",
      "Epoch 22/200\n",
      "35/47 [=====================>........] - ETA: 0s - loss: 0.1397 - accuracy: 0.9446\n",
      "Epoch 22: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1348 - accuracy: 0.9526 - val_loss: 0.1559 - val_accuracy: 0.9466\n",
      "Epoch 23/200\n",
      "36/47 [=====================>........] - ETA: 0s - loss: 0.1150 - accuracy: 0.9618\n",
      "Epoch 23: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9567 - val_loss: 0.1529 - val_accuracy: 0.9466\n",
      "Epoch 24/200\n",
      "44/47 [===========================>..] - ETA: 0s - loss: 0.1182 - accuracy: 0.9560\n",
      "Epoch 24: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1220 - accuracy: 0.9553 - val_loss: 0.1540 - val_accuracy: 0.9466\n",
      "Epoch 25/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.1510 - accuracy: 0.9476\n",
      "Epoch 25: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9486 - val_loss: 0.1538 - val_accuracy: 0.9466\n",
      "Epoch 26/200\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9608\n",
      "Epoch 26: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1113 - accuracy: 0.9608 - val_loss: 0.1510 - val_accuracy: 0.9466\n",
      "Epoch 27/200\n",
      "36/47 [=====================>........] - ETA: 0s - loss: 0.1117 - accuracy: 0.9653\n",
      "Epoch 27: val_accuracy did not improve from 0.94656\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1083 - accuracy: 0.9648 - val_loss: 0.1499 - val_accuracy: 0.9466\n",
      "Epoch 28/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.1486 - accuracy: 0.9561\n",
      "Epoch 28: val_accuracy improved from 0.94656 to 0.95420, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9540 - val_loss: 0.1306 - val_accuracy: 0.9542\n",
      "Epoch 29/200\n",
      "34/47 [====================>.........] - ETA: 0s - loss: 0.1142 - accuracy: 0.9596\n",
      "Epoch 29: val_accuracy did not improve from 0.95420\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9540 - val_loss: 0.1354 - val_accuracy: 0.9542\n",
      "Epoch 30/200\n",
      "38/47 [=======================>......] - ETA: 0s - loss: 0.1476 - accuracy: 0.9523\n",
      "Epoch 30: val_accuracy improved from 0.95420 to 0.96183, saving model to TF-Models/weights.best.hdf5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1385 - accuracy: 0.9540 - val_loss: 0.1279 - val_accuracy: 0.9618\n",
      "Epoch 31/200\n",
      "31/47 [==================>...........] - ETA: 0s - loss: 0.1142 - accuracy: 0.9617\n",
      "Epoch 31: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1176 - accuracy: 0.9635 - val_loss: 0.1568 - val_accuracy: 0.9466\n",
      "Epoch 32/200\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9648\n",
      "Epoch 32: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1167 - accuracy: 0.9648 - val_loss: 0.1345 - val_accuracy: 0.9542\n",
      "Epoch 33/200\n",
      "36/47 [=====================>........] - ETA: 0s - loss: 0.0993 - accuracy: 0.9653\n",
      "Epoch 33: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.1086 - accuracy: 0.9594 - val_loss: 0.1424 - val_accuracy: 0.9542\n",
      "Epoch 34/200\n",
      "34/47 [====================>.........] - ETA: 0s - loss: 0.1043 - accuracy: 0.9596\n",
      "Epoch 34: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1224 - accuracy: 0.9567 - val_loss: 0.1311 - val_accuracy: 0.9466\n",
      "Epoch 35/200\n",
      "34/47 [====================>.........] - ETA: 0s - loss: 0.1092 - accuracy: 0.9651\n",
      "Epoch 35: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9662 - val_loss: 0.1289 - val_accuracy: 0.9542\n",
      "Epoch 36/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.1005 - accuracy: 0.9679\n",
      "Epoch 36: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9689 - val_loss: 0.1276 - val_accuracy: 0.9542\n",
      "Epoch 37/200\n",
      "31/47 [==================>...........] - ETA: 0s - loss: 0.1046 - accuracy: 0.9637\n",
      "Epoch 37: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1156 - accuracy: 0.9594 - val_loss: 0.1314 - val_accuracy: 0.9618\n",
      "Epoch 38/200\n",
      "36/47 [=====================>........] - ETA: 0s - loss: 0.1132 - accuracy: 0.9618\n",
      "Epoch 38: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9635 - val_loss: 0.1180 - val_accuracy: 0.9542\n",
      "Epoch 39/200\n",
      "39/47 [=======================>......] - ETA: 0s - loss: 0.0971 - accuracy: 0.9712\n",
      "Epoch 39: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9716 - val_loss: 0.1159 - val_accuracy: 0.9618\n",
      "Epoch 40/200\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9716\n",
      "Epoch 40: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9716 - val_loss: 0.1118 - val_accuracy: 0.9542\n",
      "Epoch 41/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.0961 - accuracy: 0.9713\n",
      "Epoch 41: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0928 - accuracy: 0.9729 - val_loss: 0.1091 - val_accuracy: 0.9542\n",
      "Epoch 42/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.0938 - accuracy: 0.9679\n",
      "Epoch 42: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.9689 - val_loss: 0.1088 - val_accuracy: 0.9542\n",
      "Epoch 43/200\n",
      "39/47 [=======================>......] - ETA: 0s - loss: 0.1134 - accuracy: 0.9567\n",
      "Epoch 43: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1132 - accuracy: 0.9594 - val_loss: 0.1032 - val_accuracy: 0.9542\n",
      "Epoch 44/200\n",
      "37/47 [======================>.......] - ETA: 0s - loss: 0.0863 - accuracy: 0.9696\n",
      "Epoch 44: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.9702 - val_loss: 0.1256 - val_accuracy: 0.9542\n",
      "Epoch 45/200\n",
      "41/47 [=========================>....] - ETA: 0s - loss: 0.0959 - accuracy: 0.9695\n",
      "Epoch 45: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 0.9702 - val_loss: 0.1049 - val_accuracy: 0.9618\n",
      "Epoch 46/200\n",
      "35/47 [=====================>........] - ETA: 0s - loss: 0.1169 - accuracy: 0.9643\n",
      "Epoch 46: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.1073 - accuracy: 0.9689 - val_loss: 0.1390 - val_accuracy: 0.9542\n",
      "Epoch 47/200\n",
      "39/47 [=======================>......] - ETA: 0s - loss: 0.0806 - accuracy: 0.9728\n",
      "Epoch 47: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.9729 - val_loss: 0.1100 - val_accuracy: 0.9542\n",
      "Epoch 48/200\n",
      "36/47 [=====================>........] - ETA: 0s - loss: 0.0959 - accuracy: 0.9635\n",
      "Epoch 48: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9662 - val_loss: 0.1301 - val_accuracy: 0.9542\n",
      "Epoch 49/200\n",
      "38/47 [=======================>......] - ETA: 0s - loss: 0.0924 - accuracy: 0.9638\n",
      "Epoch 49: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.9662 - val_loss: 0.1192 - val_accuracy: 0.9542\n",
      "Epoch 50/200\n",
      "47/47 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9662\n",
      "Epoch 50: val_accuracy did not improve from 0.96183\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.0915 - accuracy: 0.9662 - val_loss: 0.1011 - val_accuracy: 0.9618\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add a checkpoint callback to store the checkpoint that has the highest validation accuracy.\n",
    "checkpoint_path = \"TF-Models/weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "# Start training\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the training history to see whether you're overfitting.\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1070 - accuracy: 0.9794\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the TEST dataset\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Average       0.92      0.98      0.95        46\n",
      "         Bad       1.00      1.00      1.00       166\n",
      "        Good       0.96      0.87      0.92        31\n",
      "\n",
      "    accuracy                           0.98       243\n",
      "   macro avg       0.96      0.95      0.95       243\n",
      "weighted avg       0.98      0.98      0.98       243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classify pose in the TEST dataset using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the prediction result to class name\n",
    "y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]\n",
    "y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]\n",
    "\n",
    "# Print the classification report\n",
    "print('\\nClassification Report:\\n', classification_report(y_true_label, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/kj/z6f2hnq53ll7t2pf5jdlhc3h0000gn/T/tmpffos_7vj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/kj/z6f2hnq53ll7t2pf5jdlhc3h0000gn/T/tmpffos_7vj/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 26KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:59:04.013932: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-01-29 22:59:04.014440: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-01-29 22:59:04.016264: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/kj/z6f2hnq53ll7t2pf5jdlhc3h0000gn/T/tmpffos_7vj\n",
      "2023-01-29 22:59:04.020103: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-01-29 22:59:04.020132: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/kj/z6f2hnq53ll7t2pf5jdlhc3h0000gn/T/tmpffos_7vj\n",
      "2023-01-29 22:59:04.040341: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-01-29 22:59:04.131161: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/kj/z6f2hnq53ll7t2pf5jdlhc3h0000gn/T/tmpffos_7vj\n",
      "2023-01-29 22:59:04.159787: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 144532 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Convert pose classification model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "print('Model size: %dKB' % (len(tflite_model) / 1024))\n",
    "\n",
    "with open(CLASSIFIER, 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "with open(LABELS, 'w') as f:\n",
    "  f.write('\\n'.join(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of TFLite model: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(interpreter, X, y_true):\n",
    "  \"\"\"Evaluates the given TFLite model and return its accuracy.\"\"\"\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on all given poses.\n",
    "  y_pred = []\n",
    "  for i in range(len(y_true)):\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = X[i: i + 1].astype('float32')\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the class with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    predicted_label = np.argmax(output()[0])\n",
    "    y_pred.append(predicted_label)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  y_pred = keras.utils.to_categorical(y_pred)\n",
    "  return accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Evaluate the accuracy of the converted TFLite model\n",
    "classifier_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "classifier_interpreter.allocate_tensors()\n",
    "print('Accuracy of TFLite model: %s' %\n",
    "      evaluate_model(classifier_interpreter, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3ad39121b9488888172f994e88cbc8872a978658ded75dd1485f0556f0e85c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
